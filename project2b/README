NAME: Xilai Zhang
EMAIL: xilaizhang@g.ucla.edu
ID: 804796478

Makefile: support build,tests,graphs,dist,clean,profile. To make each option 
usable by itself, they have to be executed in a specific sequence. 
For example, make
tests is required to genereate the data before calling make graphs.
SortedList.c: the implementation of SortedList.h provided by the professor.
lab2_list.c: use multiple threads to insert, lookup, and delete required 
number of list, in order to observe different locking and yielding mechanisms.
.csv files: the results of running binary files generated by .c files.
.png files: graphs required by the professor.
.gp files: file to generate gnuplot.
profile.out:execution profiling report showing where time was spent in the 
un-partitioned spin-lock implementation. 

question 2.3.1

for one thread and spin lock, I think most of the time is spent on list 
operations because there is no contention. for one thread and mutex, most
of the time would be spent on list operations if list size is large. If list
size is small, we can't determine which one is more time consuming. for two
threads and spin lock, half and half, because one thread is doing work, the
other thread is spinning. For two threads and mutex, if list size is large
list operations take more time. If list size is small, possibly mutex take
more cpu time.
for high thread spin lock, it should be on spin lock because acquiring the 
lock will be very expensive.
for high thread mutex lock, it should be on list operations.

question 2.3.2

the lines that checks spin lock condition and try to acquire the lock consume
most of the cycles.
It becomes expensive because the more threads we have, the longer time each
thread will be spinning to wait for the lock. And spinning is very expensive 
and wastes a lot of CPU cycles.

question 2.3.3

since only one thread enters the critical section, the more thread we have,
the more thread will have to wait for the lock. When we sum these waiting 
time up, we can see that the lock-wait time rise dramatically.
the more thread we have, the more context switches we will have to perform,
and the more contention we will have. Thus the completion time increases a 
little.
wait time can go faster than completion time because, we are counting wall time
 for completion time. While for the wait time, we sum up the time spent in
each CPU cores. 

question 2.3.4
as the lists increase, the throughoutput also increases, because there will
be less contention in each list.
the throughoutput will eventually saturate, and will not keep increasing.
Depending on the list or cpu cores we have, eventually, no two threads will be 
modifying the same list, and this becomes our upper bound. 
Also, as sublist size gets smaller and smaller, there will be a time when we 
no longer searches the list, and that is our upper bound.
It is not reasonable to suggest this way. the partitioned list increase 
throughoutput because it reduces the size of lists and thus reduce contention.
While a single list still have all threads modifying the same list. 

sources used:
Linux man pages of unfamiliar functions

note: 
Sometimes I fail the test script because the iterations are 200 times larger
than usual. I guess that is a linux server glitch?
I posted a question about this on piazza, see question @351

for non cs major student like me, to get gnuplot 5.0 instead of 4.6
i did: export path=/usr/local/cs/bin:$PATH

I used ./lab2_list instead of gnuplot lab2_list in my make graphs, 
because I am a EE student and doing the later will default gnuplot to 4.6.